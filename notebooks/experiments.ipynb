{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DMSQQuantizer:\n",
    "    def __init__(self, model, precision_map, device='cpu'):\n",
    "        \"\"\"\n",
    "        Initialize the DMSQ quantizer.\n",
    "\n",
    "        Parameters:\n",
    "        - model: The pre-trained PyTorch model to be quantized.\n",
    "        - precision_map: Dictionary mapping layers to desired bit precision.\n",
    "          Example: {'layer1': 8, 'layer2': 4, 'default': 8}.\n",
    "        - device: Device to run the quantized model on (e.g., 'cpu', 'cuda').\n",
    "        \"\"\"\n",
    "        self.model = model.to(device)\n",
    "        self.precision_map = precision_map\n",
    "        self.device = device\n",
    "        self.scaling_factors = {}\n",
    "        self.zero_points = {}\n",
    "        self.original_weights = {}\n",
    "        self.sensitivity_scores = {}\n",
    "\n",
    "    def calculate_scaling(self, tensor, bits):\n",
    "        \"\"\"\n",
    "        Calculate scaling factor and zero point for quantization.\n",
    "        \"\"\"\n",
    "        qmin = - (2 ** (bits - 1))\n",
    "        qmax = (2 ** (bits - 1)) - 1\n",
    "        min_val, max_val = tensor.min(), tensor.max()\n",
    "\n",
    "        scale = (max_val - min_val) / (qmax - qmin)\n",
    "        zero_point = qmin - min_val / scale\n",
    "        zero_point = torch.round(zero_point).to(torch.int)\n",
    "        \n",
    "        return scale, zero_point\n",
    "\n",
    "    def quantize_tensor(self, tensor, scale, zero_point, bits):\n",
    "        \"\"\"\n",
    "        Quantize a tensor to the specified bit precision.\n",
    "        \"\"\"\n",
    "        qmin = - (2 ** (bits - 1))\n",
    "        qmax = (2 ** (bits - 1)) - 1\n",
    "\n",
    "        quantized = torch.round(tensor / scale + zero_point)\n",
    "        quantized = torch.clamp(quantized, qmin, qmax)\n",
    "        return quantized\n",
    "\n",
    "    def dequantize_tensor(self, quantized_tensor, scale, zero_point):\n",
    "        \"\"\"\n",
    "        Dequantize a tensor from quantized representation.\n",
    "        \"\"\"\n",
    "        return scale * (quantized_tensor - zero_point)\n",
    "\n",
    "    def layer_sensitivity_analysis(self, inputs, criterion):\n",
    "        \"\"\"\n",
    "        Analyze the sensitivity of each layer to precision loss.\n",
    "        This step evaluates the impact of precision scaling on task performance.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        original_output = self.model(inputs)\n",
    "        loss_original = criterion(original_output, original_output.clone().detach())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for name, module in self.model.named_modules():\n",
    "                if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "                    # Temporarily quantize the weights\n",
    "                    precision = self.precision_map.get(name, self.precision_map.get('default', 8))\n",
    "                    scale, zero_point = self.calculate_scaling(module.weight, precision)\n",
    "                    quantized_weight = self.quantize_tensor(module.weight, scale, zero_point, precision)\n",
    "                    dequantized_weight = self.dequantize_tensor(quantized_weight, scale, zero_point)\n",
    "                    \n",
    "                    # Replace and evaluate\n",
    "                    original_weight = module.weight.clone()\n",
    "                    module.weight = nn.Parameter(dequantized_weight)\n",
    "                    quantized_output = self.model(inputs)\n",
    "                    loss_quantized = criterion(original_output, quantized_output)\n",
    "                    \n",
    "                    # Restore original weights and record sensitivity\n",
    "                    module.weight = nn.Parameter(original_weight)\n",
    "                    self.sensitivity_scores[name] = loss_quantized.item() - loss_original.item()\n",
    "\n",
    "    def workload_optimization(self, target_memory, target_latency, inputs):\n",
    "        \"\"\"\n",
    "        Optimize precision map based on memory and latency constraints.\n",
    "        \n",
    "        Parameters:\n",
    "        - target_memory: Target memory usage in bytes.\n",
    "        - target_latency: Target latency in milliseconds.\n",
    "        - inputs: Sample input tensor to profile the model.\n",
    "        \"\"\"\n",
    "        # Mock profiling values (replace with actual profiling logic if available)\n",
    "        memory_usage = 1e8  # Example: 100MB\n",
    "        latency_usage = 10  # Example: 10ms\n",
    "    \n",
    "        # Ensure these are numerical\n",
    "        memory_usage = float(memory_usage)\n",
    "        latency_usage = float(latency_usage)\n",
    "    \n",
    "        # Adjust precision_map dynamically (naive approach here, refine as needed)\n",
    "        for name in self.sensitivity_scores:\n",
    "            if memory_usage > target_memory or latency_usage > target_latency:\n",
    "                self.precision_map[name] = max(4, self.precision_map.get(name, 8) - 1)\n",
    "    \n",
    "            # Update memory and latency for demonstration purposes\n",
    "            memory_usage *= 0.9  # Simulate reduced memory usage\n",
    "            latency_usage *= 0.95  # Simulate reduced latency\n",
    "\n",
    "\n",
    "    def quantize_model(self):\n",
    "        \"\"\"\n",
    "        Apply quantization to the model based on precision_map and sensitivity scores.\n",
    "        \"\"\"\n",
    "        for name, module in self.model.named_modules():\n",
    "            precision = self.precision_map.get(name, self.precision_map.get('default', 8))\n",
    "            if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "                with torch.no_grad():\n",
    "                    # Backup original weights\n",
    "                    self.original_weights[name] = module.weight.clone()\n",
    "                    \n",
    "                    # Quantize weights\n",
    "                    scale, zero_point = self.calculate_scaling(module.weight, precision)\n",
    "                    self.scaling_factors[name] = scale\n",
    "                    self.zero_points[name] = zero_point\n",
    "\n",
    "                    quantized_weight = self.quantize_tensor(module.weight, scale, zero_point, precision)\n",
    "                    module.weight = nn.Parameter(self.dequantize_tensor(quantized_weight, scale, zero_point))\n",
    "\n",
    "    def quantize_activations(self, activation, name, bits):\n",
    "        \"\"\"\n",
    "        Quantize activations during the forward pass.\n",
    "        \"\"\"\n",
    "        scale, zero_point = self.calculate_scaling(activation, bits)\n",
    "        quantized_activation = self.quantize_tensor(activation, scale, zero_point, bits)\n",
    "        return self.dequantize_tensor(quantized_activation, scale, zero_point)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"\n",
    "        Forward pass through the quantized model with activation quantization.\n",
    "        \"\"\"\n",
    "        def hook(module, input, output):\n",
    "            if module in self.sensitivity_scores:  # Quantize activation for relevant layers\n",
    "                precision = self.precision_map.get(module, self.precision_map.get('default', 8))\n",
    "                return self.quantize_activations(output, module, precision)\n",
    "            return output\n",
    "\n",
    "        # Register hooks for activation quantization\n",
    "        handles = []\n",
    "        for name, module in self.model.named_modules():\n",
    "            handles.append(module.register_forward_hook(hook))\n",
    "        \n",
    "        # Perform forward pass\n",
    "        output = self.model(input_tensor)\n",
    "\n",
    "        # Remove hooks\n",
    "        for handle in handles:\n",
    "            handle.remove()\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Epoch 1, Loss: 4.1537\n",
      "Epoch 2, Loss: 3.2560\n",
      "Epoch 3, Loss: 1.9285\n",
      "Epoch 4, Loss: 1.5006\n",
      "Initial Test Accuracy: 0.5443\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "\n",
    "# Data loader for MNIST\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Load the full training dataset\n",
    "full_train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Subset the dataset to use only 100 samples\n",
    "subset_indices = torch.arange(100)  # Use the first 100 samples\n",
    "train_subset = torch.utils.data.Subset(full_train_dataset, subset_indices)\n",
    "\n",
    "# Create DataLoader for the subset\n",
    "train_loader = torch.utils.data.DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Test DataLoader remains the same\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=False, transform=transform),\n",
    "    batch_size=64, shuffle=False\n",
    ")\n",
    "\n",
    "# Define the model, loss, and optimizer\n",
    "device = torch.device('mps' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)  # Removed 'activation'\n",
    "        self.relu = nn.ReLU()  # Define ReLU as a separate layer\n",
    "        self.fc = nn.Linear(26 * 26 * 32, 10)  # Fully connected layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)  # Apply ReLU activation\n",
    "        x = x.view(-1, 26 * 26 * 32)  # Flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the model...\")\n",
    "for epoch in range(1, 5):  # Train for 3 epochs\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "def test_model(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "accuracy = test_model(model, test_loader)\n",
    "print(f\"Initial Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized Test Accuracy: 0.5443\n"
     ]
    }
   ],
   "source": [
    "# Define precision map\n",
    "precision_map = {\n",
    "    'conv1': 8,  # Higher precision for sensitive layers\n",
    "    'conv2': 4,  # Lower precision for less sensitive layers\n",
    "    'fc1': 8,    # High precision for dense layers\n",
    "    'fc2': 8,    # High precision for output\n",
    "    'default': 8  # Default precision\n",
    "}\n",
    "\n",
    "# Initialize the DMSQ quantizer\n",
    "quantizer = DMSQQuantizer(model, precision_map, device=device)\n",
    "\n",
    "# Sensitivity analysis\n",
    "dummy_input = torch.randn(1, 1, 28, 28).to(device)\n",
    "quantizer.layer_sensitivity_analysis(dummy_input, criterion)\n",
    "\n",
    "# Workload optimization (naive example with arbitrary memory and latency targets)\n",
    "quantizer.workload_optimization(target_memory=1e9, target_latency=50, inputs=dummy_input)\n",
    "\n",
    "# Apply quantization\n",
    "quantizer.quantize_model()\n",
    "\n",
    "# Test the quantized model\n",
    "quantized_accuracy = test_model(model, test_loader)\n",
    "print(f\"Quantized Test Accuracy: {quantized_accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
