{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DMSQQuantizer:\n",
    "    def __init__(self, model, precision_map, device='cpu'):\n",
    "        \"\"\"\n",
    "        Initialize the DMSQ quantizer.\n",
    "\n",
    "        Parameters:\n",
    "        - model: The pre-trained PyTorch model to be quantized.\n",
    "        - precision_map: Dictionary mapping layers to desired bit precision.\n",
    "          Example: {'layer1': 8, 'layer2': 4, 'default': 8}.\n",
    "        - device: Device to run the quantized model on (e.g., 'cpu', 'cuda').\n",
    "        \"\"\"\n",
    "        self.model = model.to(device)\n",
    "        self.precision_map = precision_map\n",
    "        self.device = device\n",
    "        self.scaling_factors = {}\n",
    "        self.zero_points = {}\n",
    "        self.original_weights = {}\n",
    "        self.sensitivity_scores = {}\n",
    "\n",
    "    def calculate_scaling(self, tensor, bits):\n",
    "        \"\"\"\n",
    "        Calculate scaling factor and zero point for quantization.\n",
    "        \"\"\"\n",
    "        qmin = - (2 ** (bits - 1))\n",
    "        qmax = (2 ** (bits - 1)) - 1\n",
    "        min_val, max_val = tensor.min(), tensor.max()\n",
    "\n",
    "        scale = (max_val - min_val) / (qmax - qmin)\n",
    "        zero_point = qmin - min_val / scale\n",
    "        zero_point = torch.round(zero_point).to(torch.int)\n",
    "        \n",
    "        return scale, zero_point\n",
    "\n",
    "    def quantize_tensor(self, tensor, scale, zero_point, bits):\n",
    "        \"\"\"\n",
    "        Quantize a tensor to the specified bit precision.\n",
    "        \"\"\"\n",
    "        qmin = - (2 ** (bits - 1))\n",
    "        qmax = (2 ** (bits - 1)) - 1\n",
    "\n",
    "        quantized = torch.round(tensor / scale + zero_point)\n",
    "        quantized = torch.clamp(quantized, qmin, qmax)\n",
    "        return quantized\n",
    "\n",
    "    def dequantize_tensor(self, quantized_tensor, scale, zero_point):\n",
    "        \"\"\"\n",
    "        Dequantize a tensor from quantized representation.\n",
    "        \"\"\"\n",
    "        return scale * (quantized_tensor - zero_point)\n",
    "\n",
    "    def layer_sensitivity_analysis(self, inputs, criterion):\n",
    "        \"\"\"\n",
    "        Analyze the sensitivity of each layer to precision loss.\n",
    "        This step evaluates the impact of precision scaling on task performance.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        original_output = self.model(inputs)\n",
    "        loss_original = criterion(original_output, original_output.clone().detach())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for name, module in self.model.named_modules():\n",
    "                if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "                    # Temporarily quantize the weights\n",
    "                    precision = self.precision_map.get(name, self.precision_map.get('default', 8))\n",
    "                    scale, zero_point = self.calculate_scaling(module.weight, precision)\n",
    "                    quantized_weight = self.quantize_tensor(module.weight, scale, zero_point, precision)\n",
    "                    dequantized_weight = self.dequantize_tensor(quantized_weight, scale, zero_point)\n",
    "                    \n",
    "                    # Replace and evaluate\n",
    "                    original_weight = module.weight.clone()\n",
    "                    module.weight = nn.Parameter(dequantized_weight)\n",
    "                    quantized_output = self.model(inputs)\n",
    "                    loss_quantized = criterion(original_output, quantized_output)\n",
    "                    \n",
    "                    # Restore original weights and record sensitivity\n",
    "                    module.weight = nn.Parameter(original_weight)\n",
    "                    self.sensitivity_scores[name] = loss_quantized.item() - loss_original.item()\n",
    "\n",
    "    def workload_optimization(self, target_memory, target_latency, inputs):\n",
    "        \"\"\"\n",
    "        Optimize precision map based on memory and latency constraints.\n",
    "        \n",
    "        Parameters:\n",
    "        - target_memory: Target memory usage in bytes.\n",
    "        - target_latency: Target latency in milliseconds.\n",
    "        - inputs: Sample input tensor to profile the model.\n",
    "        \"\"\"\n",
    "        # Mock profiling values (replace with actual profiling logic if available)\n",
    "        memory_usage = 1e8  # Example: 100MB\n",
    "        latency_usage = 10  # Example: 10ms\n",
    "    \n",
    "        # Ensure these are numerical\n",
    "        memory_usage = float(memory_usage)\n",
    "        latency_usage = float(latency_usage)\n",
    "    \n",
    "        # Adjust precision_map dynamically (naive approach here, refine as needed)\n",
    "        for name in self.sensitivity_scores:\n",
    "            if memory_usage > target_memory or latency_usage > target_latency:\n",
    "                self.precision_map[name] = max(4, self.precision_map.get(name, 8) - 1)\n",
    "    \n",
    "            # Update memory and latency for demonstration purposes\n",
    "            memory_usage *= 0.9  # Simulate reduced memory usage\n",
    "            latency_usage *= 0.95  # Simulate reduced latency\n",
    "\n",
    "\n",
    "    def quantize_model(self):\n",
    "        \"\"\"\n",
    "        Apply quantization to the model based on precision_map and sensitivity scores.\n",
    "        \"\"\"\n",
    "        for name, module in self.model.named_modules():\n",
    "            precision = self.precision_map.get(name, self.precision_map.get('default', 8))\n",
    "            if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "                with torch.no_grad():\n",
    "                    # Backup original weights\n",
    "                    self.original_weights[name] = module.weight.clone()\n",
    "                    \n",
    "                    # Quantize weights\n",
    "                    scale, zero_point = self.calculate_scaling(module.weight, precision)\n",
    "                    self.scaling_factors[name] = scale\n",
    "                    self.zero_points[name] = zero_point\n",
    "\n",
    "                    quantized_weight = self.quantize_tensor(module.weight, scale, zero_point, precision)\n",
    "                    module.weight = nn.Parameter(self.dequantize_tensor(quantized_weight, scale, zero_point))\n",
    "\n",
    "    def quantize_activations(self, activation, name, bits):\n",
    "        \"\"\"\n",
    "        Quantize activations during the forward pass.\n",
    "        \"\"\"\n",
    "        scale, zero_point = self.calculate_scaling(activation, bits)\n",
    "        quantized_activation = self.quantize_tensor(activation, scale, zero_point, bits)\n",
    "        return self.dequantize_tensor(quantized_activation, scale, zero_point)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"\n",
    "        Forward pass through the quantized model with activation quantization.\n",
    "        \"\"\"\n",
    "        def hook(module, input, output):\n",
    "            if module in self.sensitivity_scores:  # Quantize activation for relevant layers\n",
    "                precision = self.precision_map.get(module, self.precision_map.get('default', 8))\n",
    "                return self.quantize_activations(output, module, precision)\n",
    "            return output\n",
    "\n",
    "        # Register hooks for activation quantization\n",
    "        handles = []\n",
    "        for name, module in self.model.named_modules():\n",
    "            handles.append(module.register_forward_hook(hook))\n",
    "        \n",
    "        # Perform forward pass\n",
    "        output = self.model(input_tensor)\n",
    "\n",
    "        # Remove hooks\n",
    "        for handle in handles:\n",
    "            handle.remove()\n",
    "        \n",
    "        return output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
